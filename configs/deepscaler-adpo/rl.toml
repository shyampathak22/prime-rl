# DeepScaleR ADPO (GDPO + ARW) Experiment
#
# GDPO with multi-reward (per-reward normalized advantages) + adaptive weight decay
# Ungated length reward (fires independently) for adaptive decay to work

inference_gpu_ids = [0,1,2,3,4,5]
trainer_gpu_ids = [6,7]

max_steps = 1000
seq_len = 16384

[model]
name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

[wandb]
project = "adpo-ablations"
name = "adpo"

[ckpt]
interval = 100

[orchestrator]
batch_size = 1024
rollouts_per_example = 8

[orchestrator.sampling]
temperature = 0.6
max_tokens = 8192

[[orchestrator.env]]
id = "deepscaler-multireward"
name = "deepscaler"
# gated_length_reward=false: length reward fires independently (for adaptive decay)
args = { length_threshold_tokens = 4000, gated_length_reward = false, math_verify_max_workers = 128, math_verify_timeout = 60 }
# GDPO: per-reward normalization before aggregation
reward_keys = ["correct_answer", "length_reward"]
reward_weights = [1.0, 1.0]

# Adaptive weight decay: both rewards can decay to 0.1
[orchestrator.env.adaptive_weights]
enabled = true
ema_alpha = 0.1
saturation_threshold = 0.95
decay_exponent = 2.0
min_weights = [0.1, 0.1]
recovery_rate = 0.01

[orchestrator.advantage]
batch_normalize = true
std_eps = 1e-8

[trainer.model.ac]

[inference.parallel]
dp = 6
